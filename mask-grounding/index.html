<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Mask Grounding is an innovative auxiliary task that can be added during training to improve model performance for Referring Image Segmentation (RIS)">
  <meta name="keywords" content="Mask, Grounding, Referring, Image, Segmentation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Mask Grounding for Referring Image Segmentation</title>

  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Mask Grounding for Referring Image Segmentation</h1>
          <h2 class="title is-5 publication-title">CVPR 2024</h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://yxchng.github.io/projects/mask-grounding">Yong Xien Chng</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://yxchng.github.io/projects/mask-grounding">Henry Zheng</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://www.yizenghan.top">Yizeng Han</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://xcqiu.github.io">Xuchong Qiu</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="http://www.gaohuang.net">Gao Huang</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Tsinghua University</span>
            <span class="author-block"><sup>2</sup>Bosch Corporate Research</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2312.12198"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2312.12198"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://yxchng.github.io/projects/mask-grounding"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/teaser.png" alt="empty">
      <h2 class="subtitle has-text-centered">
        <br>
        <b><span class="dnerf">Mask Grounding</span> is an innovative auxiliary task that can significantly improve the performance of existing Referring Image Segmentation algorithms by explicitly teaching these models to learn fine-grained visual grounding in their language features. Specifically, during training, these models are given randomly masked textual tokens and has to learn to predict their identities based on the surrounding textual, visual and segmentation information.</b>
      </h2>
    </div>
  </div>
</section>


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Referring Image Segmentation (RIS) is a challenging
            task that requires an algorithm to segment objects referred
            by free-form language expressions. Despite significant
            progress in recent years, most state-of-the-art (SOTA) methods 
            still suffer from considerable language-image modality
            gap at the pixel and word level.
          </p>
          <p>
            These methods generally
            1) rely on sentence-level language features for language-image alignment and 
            2) lack explicit training supervision
            for fine-grained visual grounding. Consequently, they exhibit weak 
            object-level correspondence between visual and
            language features. Without well-grounded features, prior
            methods struggle to understand complex expressions that
            require strong reasoning over relationships among multiple
            objects, especially when dealing with rarely used or ambiguous clauses.
          </p>
          <p>
            To tackle this challenge, we introduce a
            novel Mask Grounding auxiliary task that significantly improves 
            visual grounding within language features, by explicitly teaching the model to 
            learn fine-grained correspondence between masked textual tokens and their matching
            visual objects. Mask Grounding can be directly used on
            prior RIS methods and consistently bring improvements.
            Furthermore, to holistically address the modality gap, we
            also design a cross-modal alignment loss and an accompanying alignment module. 
            These additions work synergistically with Mask Grounding. 
          </p>
          <p>
            With all these techniques,
            our comprehensive approach culminates in MagNet (Mask-grounded Network), 
            an architecture that significantly outperforms prior arts on three key benchmarks (RefCOCO,
            RefCOCO+ and G-Ref), demonstrating our method’s effectiveness in addressing current limitations of RIS algorithms.
            Our code and pre-trained weights will be released.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
          <img src="./static/images/method.png" alt="empty">
          <br>
          <br>
          <p>
            In this figure, we show the overview of Mask Grounding. To perform this task, we first use a MLP-based Mask Encoder to encode center-coordinates of segmentation masks. Then, we randomly mask textual tokens in language inputs before extracting their features. Finally, we pass the encoded language, image and mask features to a Transformer-based Masked Token Predictor to perform masked token prediction using cross-entropy loss. We use the large-scale BERT vocabulary as our word class list, which is generally accepted to have open-vocabulary capability.
          </p>
        </div>
      </div>
    </div>
    <!--/ Method. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Main Results</h2>
        <div class="content has-text-justified">
          <img src="./static/images/table1.png" alt="empty">
          <br>
          <br>
          <p>
            In this table, we compare our method with other leading RIS methods using the oIoU metric and show that it achieves multiple new state-of-the-art results. <i> Single dataset </i> refers to strictly following the predefined train/test splits of the original RefCOCO, RefCOCO+ and G-Ref datasets. <i> Multiple datasets </i> refers to combining the train splits from these 3 datasets with test images removed to prevent data leakage. <i> Extra datasets </i> refers to using additional data beyond RefCOCO, RefCOCO+ and G-Ref. † indicates models that use extra datasets. ‡ indicates that our model only uses multiple datasets. Bold indicates best.
          </p>
        </div>
      </div>
    </div>

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Visualizations</h2>
        <div class="content has-text-justified">
          <img src="./static/images/visualizations.png" alt="empty">
          <br>
          <br>
          <p>
            In this figure, we show some visualizations of our network's predictions. Compared to one of the state-of-the-art method, LAVT, our method performs much better in various complex scenerios, suggesting its impressive capability to reason about various complex visual-object relationships.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{chng2023mask,
  author    = {Chng, Yong Xien and Zheng, Henry and Han, Yizeng and Qiu, Xuchong and Huang, Gao},
  title     = {Mask Grounding for Referring Image Segmentation},
  booktitle = {CVPR},
  year      = {2024},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Thanks for the project template by <a href="https://github.com/nerfies/nerfies.github.io"> Nerfies Project</a>. 
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
